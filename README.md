# Batch_Norm_Udacity_Mod
Modified Udacity Pytorch Batch Norm example to compare BN after Relu activation.

The original notebook:
https://github.com/udacity/deep-learning-v2-pytorch/blob/master/batch-norm/Batch_Normalization.ipynb

Added a more concise and clear replication of the notebook.

There is clear evidence that a BN layer after a Relu layer is better, at least for the simple MNIST example and the Relu activation.

Worth testing other activation functions and other datasets and other networks, something to keep in mind generally. 

